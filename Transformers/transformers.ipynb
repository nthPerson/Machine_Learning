{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers\n",
    "\n",
    "In this example we implement a multiple head attension sub-layer in a transformer encoder and perform natural language processing (NLP) tasks using a transformer-based model.\n",
    "\n",
    "Import packages"
   ],
   "id": "30bf9279e25c1fe7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:26:43.144256Z",
     "start_time": "2025-01-14T05:26:40.648965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "from torch.nn.functional import cosine_similarity"
   ],
   "id": "a907f1c5f7905c1a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multiple Head Attention Sub-Layer\n",
    "\n",
    "Initialize Input Data"
   ],
   "id": "7c155d5bc8df20c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:28:20.993334Z",
     "start_time": "2025-01-14T05:28:20.986867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(0) # Do not remove this line\n",
    "\n",
    "d_model = 512\n",
    "m_inputs = 3\n",
    "\n",
    "x = np.random.rand(m_inputs, d_model)\n",
    "\n",
    "print('x:', x)\n",
    "print('x.shape:', x.shape)"
   ],
   "id": "6227aa2429565308",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\n",
      " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\n",
      " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\n",
      "x.shape: (3, 512)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create Matrices for Query, Key, and Value",
   "id": "1fd514d37281e2c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:39:04.937985Z",
     "start_time": "2025-01-14T05:39:04.924539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_heads = 8\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "# Create an empty tensor W with the correct dimensions\n",
    "W = torch.empty((d_model, d_k))\n",
    "\n",
    "# Create Query matrix\n",
    "torch.manual_seed(0)\n",
    "# Randomly initialize the values in the tensor\n",
    "nn.init.xavier_uniform_(W)\n",
    "# Copy to numpy array\n",
    "W_query = W.data.numpy()\n",
    "# Calculate query matrix\n",
    "Q = x @ W_query\n",
    "\n",
    "# Repeat for Key matrix\n",
    "torch.manual_seed(1)\n",
    "W = torch.empty((d_model, d_k))\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_key = W.data.numpy()\n",
    "K = x @ W_key\n",
    "\n",
    "# Repeat for Value matrix\n",
    "torch.manual_seed(2)\n",
    "W = torch.empty((d_model, d_k))\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_value = W.data.numpy()\n",
    "V = x @ W_value\n",
    "\n",
    "print('W_query[0,:5]:', W_query[0,:5])\n",
    "print('W_query.shape:', W_query.shape)\n",
    "print('Q[0, :5]:', Q[0,:5])\n",
    "print('Q.shape:', Q.shape)\n",
    "print('K[0,:5]', K[0,:5])\n",
    "print('K.shape', K.shape)\n",
    "print('V[0,:5]', V[0,:5])\n",
    "print('V.shape', V.shape)"
   ],
   "id": "f870a8c4bd41630b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\n",
      "W_query.shape: (512, 64)\n",
      "Q[0, :5]: [-0.22772415  0.48167861  1.48693408 -1.00410576  0.19323685]\n",
      "Q.shape: (3, 64)\n",
      "K[0,:5] [ 0.2283654  -0.65482728 -0.07202067  0.49886374  0.57045028]\n",
      "K.shape (3, 64)\n",
      "V[0,:5] [-0.44997754  0.92097362 -0.76932045  0.03289757 -0.49462588]\n",
      "V.shape (3, 64)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Compute Attention Scores and Weighted Output\n",
    "\n",
    "Attention scores are calculated with the following formula:\n",
    "\\begin{equation}\n",
    "Attention(Q, K) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})\n",
    "\\end{equation}\n",
    "\n",
    "in which $\\sqrt{d_k}$ is used for normalization purposes."
   ],
   "id": "777ebd0644dad332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:43:32.721935Z",
     "start_time": "2025-01-14T05:43:32.714776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = (Q @ K.T) / math.sqrt(d_k)\n",
    "\n",
    "# Normalize attention scores\n",
    "attn_scores_norm = softmax(attn_scores, axis=1)\n",
    "\n",
    "# Test\n",
    "print('attn_scores.shape:', attn_scores.shape)\n",
    "print('Unnormalized attn_scores:', attn_scores)\n",
    "print('Normalized attn_scores:', attn_scores_norm)"
   ],
   "id": "e560f28a3e1552f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores.shape: (3, 3)\n",
      "Unnormalized attn_scores: [[-0.75497307 -0.97036233 -0.85112729]\n",
      " [ 0.23777018 -0.70730381 -0.37639239]\n",
      " [ 0.21608578 -0.73905372 -0.89881112]]\n",
      "Normalized attn_scores: [[0.36838498 0.29700212 0.33461289]\n",
      " [0.51820328 0.20140013 0.2803966 ]\n",
      " [0.58387084 0.22464925 0.19147991]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compute Weighted Output",
   "id": "2d78d13818b46d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:44:36.189340Z",
     "start_time": "2025-01-14T05:44:36.183485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weighted_output = attn_scores_norm @ V\n",
    "\n",
    "# Test\n",
    "print('weighted_output[0,:5]:', weighted_output[0,:5])\n",
    "print('weighted_output.shape:', weighted_output.shape)"
   ],
   "id": "bb197a6b1b9a500b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_output[0,:5]: [-0.37040031  0.493314   -0.78595572  0.09711595 -0.33551551]\n",
      "weighted_output.shape: (3, 64)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformer-Based NLP Tasks\n",
    "\n",
    "Install `transformers` package"
   ],
   "id": "c79bece0f233e490"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:46:15.614122Z",
     "start_time": "2025-01-14T05:46:12.235527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ],
   "id": "c79d792f345cd7a2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenize Inputs",
   "id": "369bc6cd05c53511"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:46:33.469998Z",
     "start_time": "2025-01-14T05:46:33.459469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic\n",
    "resources for human beings. From a thermodynamic point of view, any energy conversion systems\n",
    "that receive energy from the sun and/or dissipate energy to the universe are heat engines with\n",
    "photons as the \"working fluid\" and can be analyzed using the concept of entropy. While entropy\n",
    "analysis provides a particularly convenient way to understand the efficiency limits, it is typically\n",
    "taught in the context of thermodynamic cycles among quasi-equilibrium states and its\n",
    "generalization to solar energy conversion systems running in a continuous and non-equilibrium\n",
    "fashion is not straightforward. In this educational article, we present a few examples to illustrate\n",
    "how the concept of photon entropy, combined with the radiative transfer equation, can be used to\n",
    "analyze the local entropy generation processes and the efficiency limits of different solar energy\n",
    "conversion systems. We provide explicit calculations for the local and total entropy generation\n",
    "rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily\n",
    "reproduced by students. We further discuss the connection between the entropy generation and the\n",
    "device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical\n",
    "efficiency limit.\"\"\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(len(text.split()))\n",
    "print(encoded_input['input_ids'].shape)"
   ],
   "id": "bda3c2aadbfe14a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "torch.Size([1, 275])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Output Word Vectors from BERT",
   "id": "2953f83c622e63d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:48:13.289369Z",
     "start_time": "2025-01-14T05:48:13.002760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = model(**encoded_input)\n",
    "\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    "\n",
    "input_ids_pt = encoded_input['input_ids']\n",
    "input_ids_list = input_ids_pt.tolist()[0]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "\n",
    "print(input_ids_list[:10])\n",
    "print(input_tokens[:10])"
   ],
   "id": "45158bcf8e73fa5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 275, 768])\n",
      "[101, 1996, 2980, 2791, 1997, 1996, 3103, 1998, 1996, 3147]\n",
      "['[CLS]', 'the', 'hot', '##ness', 'of', 'the', 'sun', 'and', 'the', 'cold']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Find Output Vectors That Correspond to \"entropy\"",
   "id": "59fcd1966ec30c1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:49:21.809205Z",
     "start_time": "2025-01-14T05:49:21.798839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectors = []\n",
    "for i, token in enumerate(input_tokens):\n",
    "    if token == \"entropy\":\n",
    "        vectors.append(last_hidden_state[0][i])\n",
    "\n",
    "print('Number of \"entropy\":', len(vectors))\n",
    "\n",
    "matches = [torch.allclose(vectors[i], vectors[i+1]) for i in range(len(vectors)-1)]\n",
    "print(f'Do they have the same value? {matches}')"
   ],
   "id": "76830c31a32d1d25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"entropy\": 6\n",
      "Do they have the same value? [False, False, False, False, False]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Obtain Sentence Vectors from BERT",
   "id": "cc33b40cf5f6466e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:50:06.651991Z",
     "start_time": "2025-01-14T05:50:06.646154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = text.replace('\\n', ' ').split('.')\n",
    "sentences = [s.strip() + '.' for s in sentences if len(s.strip())>0] # Some cleaning work\n",
    "\n",
    "print(f'Resulting in {len(sentences)} sentences:')\n",
    "print(sentences)"
   ],
   "id": "d69fd5770a893097",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting in 6 sentences:\n",
      "['The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic resources for human beings.', 'From a thermodynamic point of view, any energy conversion systems that receive energy from the sun and/or dissipate energy to the universe are heat engines with photons as the \"working fluid\" and can be analyzed using the concept of entropy.', 'While entropy analysis provides a particularly convenient way to understand the efficiency limits, it is typically taught in the context of thermodynamic cycles among quasi-equilibrium states and its generalization to solar energy conversion systems running in a continuous and non-equilibrium fashion is not straightforward.', 'In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.', 'We provide explicit calculations for the local and total entropy generation rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily reproduced by students.', 'We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenize Example Sentences",
   "id": "104d763d2027ad56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:50:38.139700Z",
     "start_time": "2025-01-14T05:50:38.125114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_sentences = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(encoded_sentences['input_ids'].shape)\n",
    "print(encoded_sentences['input_ids'][0,:])\n",
    "print(encoded_sentences['input_ids'][1,:])"
   ],
   "id": "1ffdcfb74418ebf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57])\n",
      "tensor([  101,  1996,  2980,  2791,  1997,  1996,  3103,  1998,  1996,  3147,\n",
      "         2791,  1997,  1996,  6058,  2686,  2024,  1999, 10288, 13821,  3775,\n",
      "         3468,  1996, 10867,  7716, 18279,  7712,  4219,  2005,  2529,  9552,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([  101,  2013,  1037,  1996, 10867,  7716, 18279,  7712,  2391,  1997,\n",
      "         3193,  1010,  2151,  2943,  7584,  3001,  2008,  4374,  2943,  2013,\n",
      "         1996,  3103,  1998,  1013,  2030,  4487, 18719, 17585,  2943,  2000,\n",
      "         1996,  5304,  2024,  3684,  5209,  2007, 26383,  2015,  2004,  1996,\n",
      "         1000,  2551,  8331,  1000,  1998,  2064,  2022, 16578,  2478,  1996,\n",
      "         4145,  1997, 23077,  1012,   102,     0,     0])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Obtain Output Tensors for All Input Sentences",
   "id": "82abe600ae5f1674"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:52:03.940878Z",
     "start_time": "2025-01-14T05:52:03.585272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(**encoded_sentences)\n",
    "\n",
    "print(outputs['last_hidden_state'].shape)\n",
    "\n",
    "# Note that the first dimension of model output is batch size\n",
    "print(outputs['last_hidden_state'][0].shape)"
   ],
   "id": "6a22459e0ced8a68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57, 768])\n",
      "torch.Size([57, 768])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Represent Meaning of Sentence Using Input Token [CLS]",
   "id": "473896530221f8d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T06:33:35.783156Z",
     "start_time": "2025-01-14T06:33:35.778158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLS_vec = outputs['last_hidden_state'][0][0]\n",
    "print(CLS_vec.shape)"
   ],
   "id": "37c2dc2b3ce97582",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Compute Cosine Similarity Between Sentences\n",
    "\n",
    "Cosine similarity can be used to determine semantic similarity between statements and sentences."
   ],
   "id": "c71b4a60ad295ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T06:35:08.400139Z",
     "start_time": "2025-01-14T06:35:08.392270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    for j in range(i+1, 6):\n",
    "        sim = cosine_similarity(outputs['last_hidden_state'][i][0], outputs['last_hidden_state'][j][0], dim=0).item()\n",
    "\n",
    "        print(f'{i} <-> {j}: {sim}')"
   ],
   "id": "d19934942deee326",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <-> 1: 0.8591639995574951\n",
      "0 <-> 2: 0.7771981358528137\n",
      "0 <-> 3: 0.7985227108001709\n",
      "0 <-> 4: 0.7754685878753662\n",
      "0 <-> 5: 0.8052164316177368\n",
      "1 <-> 2: 0.876341700553894\n",
      "1 <-> 3: 0.832162082195282\n",
      "1 <-> 4: 0.823844850063324\n",
      "1 <-> 5: 0.8492752313613892\n",
      "2 <-> 3: 0.8241375684738159\n",
      "2 <-> 4: 0.8598626852035522\n",
      "2 <-> 5: 0.8579832315444946\n",
      "3 <-> 4: 0.9018083810806274\n",
      "3 <-> 5: 0.929144024848938\n",
      "4 <-> 5: 0.9185266494750977\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T06:37:03.842425Z",
     "start_time": "2025-01-14T06:37:03.837363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print sentences with largest cosine similarity\n",
    "print(sentences[3])\n",
    "print(sentences[5])"
   ],
   "id": "7d26a6f9a108dad5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.\n",
      "We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summarize Text",
   "id": "209925a4caf8d37c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T06:37:59.212925Z",
     "start_time": "2025-01-14T06:37:52.119922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", device='cuda')\n",
    "# summarizer = pipeline(\"summarization\")\n",
    "\n",
    "print(summarizer(text, max_length=150, min_length=30))\n",
    "\n",
    "# my turn\n",
    "test_text = \"\"\" GPUs, or Graphics Processing Units, are important pieces of hardware originally designed for rendering computer graphics, primarily for games and movies. However, in recent years, GPUs have gained recognition for significantly enhancing the speed of computational processes involving neural networks.\n",
    "\n",
    "GPUs now play a pivotal role in the artificial intelligence revolution, predominantly driving rapid advancements in deep learning, computer vision, and large language models, among others. \"\"\"\n",
    "print()\n",
    "print(summarizer(test_text, max_length=50, min_length=30))"
   ],
   "id": "79e6fe827c5750b1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The hotness of the sun and the coldness of outer space are inexhaustible thermodynamic resources for human beings . From a thermodynamic point of view, any energy conversion systems that receive energy from the sun or dissipate energy to the universe are heat engines with photons as the \"working fluid\"'}]\n",
      "\n",
      "[{'summary_text': ' Graphics Processing Units are important pieces of hardware originally designed for rendering computer graphics, primarily for games and movies . In recent years, they have gained recognition for significantly enhancing the speed of computational processes involving neural networks .'}]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Perform Sentiment Analysis",
   "id": "b9f5226809aaa9ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T06:38:49.450281Z",
     "start_time": "2025-01-14T06:38:48.221714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sentiment_classifier = pipeline(\"sentiment-analysis\")\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", device='cuda')\n",
    "text2 = \"I love using transformers library for natural language processing!\"\n",
    "\n",
    "# Perform sentiment classification\n",
    "result = sentiment_classifier(text2)\n",
    "\n",
    "# Output the result\n",
    "print(result)\n",
    "\n",
    "text3 = \"I didn't like the movie. It was boring\"\n",
    "\n",
    "result = sentiment_classifier(text3)\n",
    "\n",
    "# Output the result\n",
    "print(result)\n",
    "\n",
    "my_text = \"This lab wasn't extremely difficult, but I learned a lot about how to apply pretrained models.\"\n",
    "\n",
    "result = sentiment_classifier(my_text)\n",
    "\n",
    "print(result)\n"
   ],
   "id": "873f83018120f1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9984171390533447}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.999295711517334}]\n",
      "[{'label': 'POSITIVE', 'score': 0.9930394291877747}]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f0057aa11b6ea49b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

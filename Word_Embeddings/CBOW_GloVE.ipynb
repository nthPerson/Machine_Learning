{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Continuous bag of words (CBOW) model used to predict a target word given the surrounding words. Additionally, a pre-trained GloVe embedding model is used to predict target words.\n",
    "\n",
    "Load packages"
   ],
   "id": "dd4b664a276662c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:42:47.374178Z",
     "start_time": "2025-01-14T04:42:44.681403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "from collections import Counter"
   ],
   "id": "5b40121b27f0e288",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# build_vocab Helper Function",
   "id": "3baaa9831bfa3970"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:42:47.383360Z",
     "start_time": "2025-01-14T04:42:47.379475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_vocab(words: List[str]):\n",
    "    vocab = Counter()\n",
    "    for w in words:\n",
    "        vocab[w] += 1\n",
    "    return vocab"
   ],
   "id": "745cfd80bd3cc81e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prepare Data\n",
    "\n",
    "Data consists of a list of tuples with two elements each, a list of context words and the target word. Both the context words and the target word are represented by word indices."
   ],
   "id": "329fafbe3f8b375c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T04:52:25.037625Z",
     "start_time": "2025-01-14T04:52:25.029627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONTEXT_SIZE = 3  # Define the context size -- CONTEXT_SIZE words to the left and right of the target word\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "vocab = build_vocab(raw_text)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print ('vocab_size',vocab_size)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}#index for each word in vocab\n",
    "idx_to_word = list(vocab)#list of words in the vocab\n",
    "\n",
    "word_indices = [word_to_idx[w] for w in raw_text]#Every word in the raw text has an index\n",
    "\n",
    "# Creates a data sample with CONTEXT_SIZE * 2 context words and the target word between them\n",
    "def prepare_data(word_indices):\n",
    "    data = []\n",
    "    for i in range(CONTEXT_SIZE, len(word_indices) - CONTEXT_SIZE):\n",
    "        target = word_indices[i]\n",
    "\n",
    "        # Elements to the left of the target word\n",
    "        left_context = [word_indices[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
    "        # Reverse left side list to keep words in correct order\n",
    "        left_context = left_context[::-1]\n",
    "\n",
    "        # Elements to the right of the target word\n",
    "        right_context = [word_indices[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
    "\n",
    "        # Full context\n",
    "        context = left_context + right_context\n",
    "\n",
    "        data.append((context, target))\n",
    "\n",
    "    # for d in data:\n",
    "    #     print(d)\n",
    "    return data\n",
    "\n",
    "# Test data preparation\n",
    "data = prepare_data(word_indices)\n",
    "print ('length of data', len(data))\n",
    "print('data[0]:', data[0])\n",
    "ctx, tgt = data[0]\n",
    "print('context words:', [idx_to_word[c] for c in ctx])\n",
    "print('target word:', idx_to_word[tgt])"
   ],
   "id": "59daff3fe0c81981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 49\n",
      "length of data 56\n",
      "data[0]: ([0, 1, 2, 4, 5, 6], 3)\n",
      "context words: ['We', 'are', 'about', 'study', 'the', 'idea']\n",
      "target word: to\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implement the CBOW Model",
   "id": "2d8913dc2aa2252a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:04:15.069644Z",
     "start_time": "2025-01-14T05:04:15.044416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, no_of_samples, embed_weights):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        # Create the embedding matrix\n",
    "        if embed_weights is not None:\n",
    "            self.embeds = nn.Embedding.from_pretrained(embeddings=embed_weights, freeze=False)\n",
    "        else:\n",
    "            self.embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.activation = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeds(inputs)\n",
    "        embeds = embeds.sum(dim=0).unsqueeze(0)\n",
    "        output = self.linear(embeds)\n",
    "        output = self.activation(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Test model definition\n",
    "torch.manual_seed(0)\n",
    "\n",
    "m = CBOW(vocab_size=10, embedding_dim=20, no_of_samples=3, embed_weights=None)\n",
    "test_input = torch.tensor([1,2,3], dtype=torch.long)\n",
    "\n",
    "test_output = m(test_input)\n",
    "\n",
    "print('test_output.shape', test_output.shape)\n",
    "print('test_output', test_output.data)"
   ],
   "id": "13d1d499ce1b0578",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape torch.Size([1, 10])\n",
      "test_output tensor([[-1.6878, -4.2108, -5.0252, -2.9802, -3.1362, -1.5436, -1.4120, -3.2485,\n",
      "         -1.6490, -4.5009]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expected Output\n",
    "\n",
    "test_output.shape torch.Size([1, 10])<br>\n",
    "test_output tensor([[-1.6878, -4.2108, -5.0252, -2.9802, -3.1362, -1.5436, -1.4120, -3.2485,\n",
    "         -1.6490, -4.5009]])"
   ],
   "id": "3fd0ef22081f5515"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Loop",
   "id": "d46403df426c3f2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:06:23.273720Z",
     "start_time": "2025-01-14T05:06:12.159692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE*2, embed_weights=None)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for ctx, tgt in data:\n",
    "        ctx_tensor = torch.tensor(ctx, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([tgt], dtype=torch.long)\n",
    "\n",
    "        output = model(ctx_tensor)\n",
    "\n",
    "        total_loss += loss_function(output, tgt_tensor)\n",
    "\n",
    "    # Optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training information\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        print(f'Loss within epoch {epoch}: ', total_loss.item())"
   ],
   "id": "19d7a016d393b039",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss within epoch 5:  182.17068481445312\n",
      "Loss within epoch 10:  128.0272674560547\n",
      "Loss within epoch 15:  93.20354461669922\n",
      "Loss within epoch 20:  69.58182525634766\n",
      "Loss within epoch 25:  53.30769729614258\n",
      "Loss within epoch 30:  41.98340606689453\n",
      "Loss within epoch 35:  33.983062744140625\n",
      "Loss within epoch 40:  28.213905334472656\n",
      "Loss within epoch 45:  23.947219848632812\n",
      "Loss within epoch 50:  20.70659828186035\n",
      "Loss within epoch 55:  18.183000564575195\n",
      "Loss within epoch 60:  16.173580169677734\n",
      "Loss within epoch 65:  14.542234420776367\n",
      "Loss within epoch 70:  13.195314407348633\n",
      "Loss within epoch 75:  12.06682014465332\n",
      "Loss within epoch 80:  11.109159469604492\n",
      "Loss within epoch 85:  10.287294387817383\n",
      "Loss within epoch 90:  9.574958801269531\n",
      "Loss within epoch 95:  8.952115058898926\n",
      "Loss within epoch 100:  8.403236389160156\n",
      "Loss within epoch 105:  7.916140556335449\n",
      "Loss within epoch 110:  7.481140613555908\n",
      "Loss within epoch 115:  7.090439796447754\n",
      "Loss within epoch 120:  6.737703323364258\n",
      "Loss within epoch 125:  6.417740821838379\n",
      "Loss within epoch 130:  6.126246452331543\n",
      "Loss within epoch 135:  5.859633922576904\n",
      "Loss within epoch 140:  5.614891052246094\n",
      "Loss within epoch 145:  5.389458656311035\n",
      "Loss within epoch 150:  5.181169509887695\n",
      "Loss within epoch 155:  4.9881591796875\n",
      "Loss within epoch 160:  4.808822154998779\n",
      "Loss within epoch 165:  4.6417694091796875\n",
      "Loss within epoch 170:  4.485795021057129\n",
      "Loss within epoch 175:  4.339841365814209\n",
      "Loss within epoch 180:  4.20297908782959\n",
      "Loss within epoch 185:  4.0743937492370605\n",
      "Loss within epoch 190:  3.953361749649048\n",
      "Loss within epoch 195:  3.8392415046691895\n",
      "Loss within epoch 200:  3.731464385986328\n",
      "Loss within epoch 205:  3.629514217376709\n",
      "Loss within epoch 210:  3.532939910888672\n",
      "Loss within epoch 215:  3.441328763961792\n",
      "Loss within epoch 220:  3.35430645942688\n",
      "Loss within epoch 225:  3.271542549133301\n",
      "Loss within epoch 230:  3.192735195159912\n",
      "Loss within epoch 235:  3.117603063583374\n",
      "Loss within epoch 240:  3.045903444290161\n",
      "Loss within epoch 245:  2.977400541305542\n",
      "Loss within epoch 250:  2.9118919372558594\n",
      "Loss within epoch 255:  2.8491859436035156\n",
      "Loss within epoch 260:  2.7891030311584473\n",
      "Loss within epoch 265:  2.731487274169922\n",
      "Loss within epoch 270:  2.6761887073516846\n",
      "Loss within epoch 275:  2.6230711936950684\n",
      "Loss within epoch 280:  2.5720109939575195\n",
      "Loss within epoch 285:  2.522887706756592\n",
      "Loss within epoch 290:  2.475594997406006\n",
      "Loss within epoch 295:  2.430034637451172\n",
      "Loss within epoch 300:  2.38611102104187\n",
      "Loss within epoch 305:  2.3437390327453613\n",
      "Loss within epoch 310:  2.3028383255004883\n",
      "Loss within epoch 315:  2.2633328437805176\n",
      "Loss within epoch 320:  2.2251534461975098\n",
      "Loss within epoch 325:  2.188234567642212\n",
      "Loss within epoch 330:  2.1525156497955322\n",
      "Loss within epoch 335:  2.1179378032684326\n",
      "Loss within epoch 340:  2.08445143699646\n",
      "Loss within epoch 345:  2.0519983768463135\n",
      "Loss within epoch 350:  2.0205392837524414\n",
      "Loss within epoch 355:  1.990025520324707\n",
      "Loss within epoch 360:  1.9604151248931885\n",
      "Loss within epoch 365:  1.931668758392334\n",
      "Loss within epoch 370:  1.903751254081726\n",
      "Loss within epoch 375:  1.8766250610351562\n",
      "Loss within epoch 380:  1.85025954246521\n",
      "Loss within epoch 385:  1.8246188163757324\n",
      "Loss within epoch 390:  1.7996795177459717\n",
      "Loss within epoch 395:  1.775408387184143\n",
      "Loss within epoch 400:  1.751782774925232\n",
      "Loss within epoch 405:  1.728772759437561\n",
      "Loss within epoch 410:  1.706361174583435\n",
      "Loss within epoch 415:  1.6845173835754395\n",
      "Loss within epoch 420:  1.6632245779037476\n",
      "Loss within epoch 425:  1.6424617767333984\n",
      "Loss within epoch 430:  1.6222095489501953\n",
      "Loss within epoch 435:  1.602449893951416\n",
      "Loss within epoch 440:  1.5831648111343384\n",
      "Loss within epoch 445:  1.5643346309661865\n",
      "Loss within epoch 450:  1.545945405960083\n",
      "Loss within epoch 455:  1.5279840230941772\n",
      "Loss within epoch 460:  1.5104337930679321\n",
      "Loss within epoch 465:  1.4932798147201538\n",
      "Loss within epoch 470:  1.4765108823776245\n",
      "Loss within epoch 475:  1.4601143598556519\n",
      "Loss within epoch 480:  1.4440748691558838\n",
      "Loss within epoch 485:  1.4283814430236816\n",
      "Loss within epoch 490:  1.4130269289016724\n",
      "Loss within epoch 495:  1.3979980945587158\n",
      "Loss within epoch 500:  1.3832839727401733\n",
      "Loss within epoch 505:  1.3688750267028809\n",
      "Loss within epoch 510:  1.3547638654708862\n",
      "Loss within epoch 515:  1.340938687324524\n",
      "Loss within epoch 520:  1.3273929357528687\n",
      "Loss within epoch 525:  1.3141145706176758\n",
      "Loss within epoch 530:  1.3011016845703125\n",
      "Loss within epoch 535:  1.2883435487747192\n",
      "Loss within epoch 540:  1.2758288383483887\n",
      "Loss within epoch 545:  1.2635571956634521\n",
      "Loss within epoch 550:  1.25151789188385\n",
      "Loss within epoch 555:  1.2397065162658691\n",
      "Loss within epoch 560:  1.2281138896942139\n",
      "Loss within epoch 565:  1.21673583984375\n",
      "Loss within epoch 570:  1.2055675983428955\n",
      "Loss within epoch 575:  1.194599986076355\n",
      "Loss within epoch 580:  1.1838297843933105\n",
      "Loss within epoch 585:  1.1732535362243652\n",
      "Loss within epoch 590:  1.1628609895706177\n",
      "Loss within epoch 595:  1.1526532173156738\n",
      "Loss within epoch 600:  1.14262056350708\n",
      "Loss within epoch 605:  1.1327619552612305\n",
      "Loss within epoch 610:  1.1230714321136475\n",
      "Loss within epoch 615:  1.113544225692749\n",
      "Loss within epoch 620:  1.104176640510559\n",
      "Loss within epoch 625:  1.094966173171997\n",
      "Loss within epoch 630:  1.0859076976776123\n",
      "Loss within epoch 635:  1.076997995376587\n",
      "Loss within epoch 640:  1.0682318210601807\n",
      "Loss within epoch 645:  1.0596067905426025\n",
      "Loss within epoch 650:  1.051119327545166\n",
      "Loss within epoch 655:  1.0427662134170532\n",
      "Loss within epoch 660:  1.034546971321106\n",
      "Loss within epoch 665:  1.0264532566070557\n",
      "Loss within epoch 670:  1.018485426902771\n",
      "Loss within epoch 675:  1.01064133644104\n",
      "Loss within epoch 680:  1.0029157400131226\n",
      "Loss within epoch 685:  0.9953064918518066\n",
      "Loss within epoch 690:  0.9878123998641968\n",
      "Loss within epoch 695:  0.9804300665855408\n",
      "Loss within epoch 700:  0.9731600284576416\n",
      "Loss within epoch 705:  0.9659906625747681\n",
      "Loss within epoch 710:  0.9589311480522156\n",
      "Loss within epoch 715:  0.9519708752632141\n",
      "Loss within epoch 720:  0.9451117515563965\n",
      "Loss within epoch 725:  0.938349723815918\n",
      "Loss within epoch 730:  0.9316833019256592\n",
      "Loss within epoch 735:  0.9251123666763306\n",
      "Loss within epoch 740:  0.9186325073242188\n",
      "Loss within epoch 745:  0.9122431874275208\n",
      "Loss within epoch 750:  0.905939519405365\n",
      "Loss within epoch 755:  0.899724543094635\n",
      "Loss within epoch 760:  0.8935933709144592\n",
      "Loss within epoch 765:  0.887544572353363\n",
      "Loss within epoch 770:  0.8815783262252808\n",
      "Loss within epoch 775:  0.8756899833679199\n",
      "Loss within epoch 780:  0.8698799014091492\n",
      "Loss within epoch 785:  0.8641457557678223\n",
      "Loss within epoch 790:  0.8584882616996765\n",
      "Loss within epoch 795:  0.8529027104377747\n",
      "Loss within epoch 800:  0.8473902940750122\n",
      "Loss within epoch 805:  0.8419492840766907\n",
      "Loss within epoch 810:  0.8365755081176758\n",
      "Loss within epoch 815:  0.8312685489654541\n",
      "Loss within epoch 820:  0.826030433177948\n",
      "Loss within epoch 825:  0.8208591938018799\n",
      "Loss within epoch 830:  0.8157496452331543\n",
      "Loss within epoch 835:  0.810703456401825\n",
      "Loss within epoch 840:  0.8057209253311157\n",
      "Loss within epoch 845:  0.8007965683937073\n",
      "Loss within epoch 850:  0.7959351539611816\n",
      "Loss within epoch 855:  0.7911278605461121\n",
      "Loss within epoch 860:  0.7863820791244507\n",
      "Loss within epoch 865:  0.7816897034645081\n",
      "Loss within epoch 870:  0.777056872844696\n",
      "Loss within epoch 875:  0.7724749445915222\n",
      "Loss within epoch 880:  0.7679487466812134\n",
      "Loss within epoch 885:  0.7634739279747009\n",
      "Loss within epoch 890:  0.7590500116348267\n",
      "Loss within epoch 895:  0.7546790838241577\n",
      "Loss within epoch 900:  0.7503584027290344\n",
      "Loss within epoch 905:  0.7460826635360718\n",
      "Loss within epoch 910:  0.7418593764305115\n",
      "Loss within epoch 915:  0.7376834154129028\n",
      "Loss within epoch 920:  0.7335495948791504\n",
      "Loss within epoch 925:  0.7294668555259705\n",
      "Loss within epoch 930:  0.7254253029823303\n",
      "Loss within epoch 935:  0.7214319705963135\n",
      "Loss within epoch 940:  0.7174814939498901\n",
      "Loss within epoch 945:  0.7135722041130066\n",
      "Loss within epoch 950:  0.7097070813179016\n",
      "Loss within epoch 955:  0.7058818340301514\n",
      "Loss within epoch 960:  0.7020972967147827\n",
      "Loss within epoch 965:  0.6983548402786255\n",
      "Loss within epoch 970:  0.6946495771408081\n",
      "Loss within epoch 975:  0.6909850239753723\n",
      "Loss within epoch 980:  0.6873605251312256\n",
      "Loss within epoch 985:  0.6837700605392456\n",
      "Loss within epoch 990:  0.6802188158035278\n",
      "Loss within epoch 995:  0.6767032742500305\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Helper Function to Get Predicted Word",
   "id": "84adc78787716af9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:08:35.781300Z",
     "start_time": "2025-01-14T05:08:35.773787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_predicted_word(model_output, idx_to_word):\n",
    "    word = idx_to_word[torch.argmax(model_output)]\n",
    "\n",
    "    return word\n",
    "\n",
    "# Test 1\n",
    "ctx_words = 'processes manipulate other things called data.'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model(ctx_tensor)\n",
    "pred = get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')\n",
    "\n",
    "# Test 2\n",
    "ctx_words = 'we conjure the of the computer'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model(ctx_tensor)\n",
    "pred = get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ],
   "id": "4d4d47365e86bb70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word is: \"abstract\"\n",
      "The predicted word is: \"spirits\"\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Use Pretrained GloVe Embeddings to Perform Target Word Prediction\n",
    "\n",
    "Load GloVe embeddings"
   ],
   "id": "fdf0689e812b456a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:11:39.814196Z",
     "start_time": "2025-01-14T05:11:29.481966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_glove_embeddings(embedding_file):\n",
    "    g_embeddings = {}\n",
    "    g_vocab = []\n",
    "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            g_vocab.append(word)\n",
    "            vector = torch.tensor([float(val) for val in values[1:]])\n",
    "            g_embeddings[word] = vector\n",
    "    return g_embeddings, g_vocab\n",
    "\n",
    "# Load pretrained GloVe embeddings and extract vocabulary\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "g_embeddings, g_vocab = load_glove_embeddings(glove_file)\n",
    "\n",
    "# Test to make sure the GloVe embeddings loaded correctly\n",
    "print ('g_vocab_size', len(g_vocab))\n",
    "\n",
    "g_word_to_idx = {word: i for i, word in enumerate(g_vocab)}\n",
    "g_idx_to_word = list(g_vocab)\n",
    "\n",
    "g_vocab_size, g_embedding_dim = len (g_vocab),50\n",
    "\n",
    "g_embedding_matrix = torch.stack(list(g_embeddings.values()))\n",
    "print ('g_embedding_matrix.shape',g_embedding_matrix.shape)\n",
    "\n",
    "# Create PyTorch embedding layer\n",
    "g_embedding_layer = nn.Embedding.from_pretrained(g_embedding_matrix)\n",
    "word = 'the'\n",
    "g_word_index = g_vocab.index(word)\n",
    "\n",
    "print ('g_word_index', g_word_index)"
   ],
   "id": "66fe42e727bd0ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_vocab_size 400001\n",
      "g_embedding_matrix.shape torch.Size([400001, 50])\n",
      "g_word_index 0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Expected Output\n",
    "\n",
    "g_vocab_size 400001<br>\n",
    "g_embedding_matrix.shape torch.Size([400001, 50])<br>\n",
    "g_word_index 0"
   ],
   "id": "d71fd68d0e265399"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate Cosine Similarity for Sample Words",
   "id": "fd237c6a4d771956"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:14:03.479202Z",
     "start_time": "2025-01-14T05:14:03.466171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_king = 'king'\n",
    "word_man ='man'\n",
    "word_woman = 'woman'\n",
    "word_queen = 'queen'\n",
    "\n",
    "\n",
    "king_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_king)]))\n",
    "man_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_man)]))\n",
    "woman_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_woman)]))\n",
    "queen_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_queen)]))\n",
    "\n",
    "word_embedding_case_1 = king_embedding - man_embedding + woman_embedding\n",
    "\n",
    "# Obtain the cosine similarity between king-man+woman to queen in \"cosine_sim1\"\n",
    "king_minus_man_plus_woman = king_embedding - man_embedding + woman_embedding\n",
    "cosine_sim1 = F.cosine_similarity(king_minus_man_plus_woman, queen_embedding)\n",
    "\n",
    "#Obtain the cosine similarity between king and queen in \"cosine_sim2\"\n",
    "cosine_sim2 = F.cosine_similarity(king_embedding, queen_embedding)\n",
    "\n",
    "print(\"Cosine sim1:\", cosine_sim1.item())\n",
    "print(\"Cosine sim2:\", cosine_sim2.item())"
   ],
   "id": "db972931be34bfd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine sim1: 0.8609580993652344\n",
      "Cosine sim2: 0.7839043736457825\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create New Embedding Matrix for 'raw_text' Vocabulary",
   "id": "99febe8bde26cf55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:16:52.797541Z",
     "start_time": "2025-01-14T05:16:52.782549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Convert word to lowercase\n",
    "    word = word.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "n_embedding_matrix = torch.zeros(len(vocab), g_embedding_matrix.shape[1])\n",
    "print ('g', g_embedding_matrix.shape)\n",
    "print ('n', n_embedding_matrix.shape)\n",
    "\n",
    "new_vocab_indices = {}\n",
    "for i, word in enumerate (vocab):\n",
    "    word=preprocess_word(word)\n",
    "    if word in g_vocab:\n",
    "        index = g_vocab.index(word)\n",
    "        n_embedding_matrix[i] = torch.tensor(g_embedding_matrix[index], dtype=torch.float)\n",
    "    else:\n",
    "        print ('This word not in GloVe',word)"
   ],
   "id": "563897147d9e67ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([400001, 50])\n",
      "n torch.Size([49, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_2224\\4261100038.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  n_embedding_matrix[i] = torch.tensor(g_embedding_matrix[index], dtype=torch.float)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Loop with Pretrained GloVe Embeddings",
   "id": "4b54cceaf5c37e85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:18:42.368805Z",
     "start_time": "2025-01-14T05:18:30.184634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "model2 = CBOW(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE*2, n_embedding_matrix)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "\n",
    "    for ctx, tgt in data:\n",
    "        ctx_tensor = torch.tensor(ctx, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([tgt], dtype=torch.long)\n",
    "\n",
    "        output = model2(ctx_tensor)\n",
    "\n",
    "        total_loss += loss_function(output, tgt_tensor)\n",
    "\n",
    "    # Optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training information\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        print(f'Loss within epoch {epoch}: ', total_loss.item())"
   ],
   "id": "52b91140101a22ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss within epoch 5:  201.59201049804688\n",
      "Loss within epoch 10:  178.20095825195312\n",
      "Loss within epoch 15:  161.12486267089844\n",
      "Loss within epoch 20:  146.86415100097656\n",
      "Loss within epoch 25:  134.70684814453125\n",
      "Loss within epoch 30:  124.19705963134766\n",
      "Loss within epoch 35:  115.00505828857422\n",
      "Loss within epoch 40:  106.8845443725586\n",
      "Loss within epoch 45:  99.64949035644531\n",
      "Loss within epoch 50:  93.15776062011719\n",
      "Loss within epoch 55:  87.29913330078125\n",
      "Loss within epoch 60:  81.98625946044922\n",
      "Loss within epoch 65:  77.14881134033203\n",
      "Loss within epoch 70:  72.72892761230469\n",
      "Loss within epoch 75:  68.67851257324219\n",
      "Loss within epoch 80:  64.95677947998047\n",
      "Loss within epoch 85:  61.5289306640625\n",
      "Loss within epoch 90:  58.36494827270508\n",
      "Loss within epoch 95:  55.4387321472168\n",
      "Loss within epoch 100:  52.7274169921875\n",
      "Loss within epoch 105:  50.210914611816406\n",
      "Loss within epoch 110:  47.87141418457031\n",
      "Loss within epoch 115:  45.693145751953125\n",
      "Loss within epoch 120:  43.66200637817383\n",
      "Loss within epoch 125:  41.76542663574219\n",
      "Loss within epoch 130:  39.992122650146484\n",
      "Loss within epoch 135:  38.331939697265625\n",
      "Loss within epoch 140:  36.77571105957031\n",
      "Loss within epoch 145:  35.3151969909668\n",
      "Loss within epoch 150:  33.94292449951172\n",
      "Loss within epoch 155:  32.65209197998047\n",
      "Loss within epoch 160:  31.436542510986328\n",
      "Loss within epoch 165:  30.290687561035156\n",
      "Loss within epoch 170:  29.209388732910156\n",
      "Loss within epoch 175:  28.188005447387695\n",
      "Loss within epoch 180:  27.222272872924805\n",
      "Loss within epoch 185:  26.30826759338379\n",
      "Loss within epoch 190:  25.44245147705078\n",
      "Loss within epoch 195:  24.621503829956055\n",
      "Loss within epoch 200:  23.842437744140625\n",
      "Loss within epoch 205:  23.10247039794922\n",
      "Loss within epoch 210:  22.399059295654297\n",
      "Loss within epoch 215:  21.72983169555664\n",
      "Loss within epoch 220:  21.092632293701172\n",
      "Loss within epoch 225:  20.485450744628906\n",
      "Loss within epoch 230:  19.906423568725586\n",
      "Loss within epoch 235:  19.353851318359375\n",
      "Loss within epoch 240:  18.82613182067871\n",
      "Loss within epoch 245:  18.321794509887695\n",
      "Loss within epoch 250:  17.83946418762207\n",
      "Loss within epoch 255:  17.37787437438965\n",
      "Loss within epoch 260:  16.935842514038086\n",
      "Loss within epoch 265:  16.51226234436035\n",
      "Loss within epoch 270:  16.106115341186523\n",
      "Loss within epoch 275:  15.716446876525879\n",
      "Loss within epoch 280:  15.342351913452148\n",
      "Loss within epoch 285:  14.98300838470459\n",
      "Loss within epoch 290:  14.637633323669434\n",
      "Loss within epoch 295:  14.305506706237793\n",
      "Loss within epoch 300:  13.985930442810059\n",
      "Loss within epoch 305:  13.678279876708984\n",
      "Loss within epoch 310:  13.381952285766602\n",
      "Loss within epoch 315:  13.09638500213623\n",
      "Loss within epoch 320:  12.821046829223633\n",
      "Loss within epoch 325:  12.555450439453125\n",
      "Loss within epoch 330:  12.299121856689453\n",
      "Loss within epoch 335:  12.05162525177002\n",
      "Loss within epoch 340:  11.812548637390137\n",
      "Loss within epoch 345:  11.581503868103027\n",
      "Loss within epoch 350:  11.358124732971191\n",
      "Loss within epoch 355:  11.142057418823242\n",
      "Loss within epoch 360:  10.932981491088867\n",
      "Loss within epoch 365:  10.73059368133545\n",
      "Loss within epoch 370:  10.534592628479004\n",
      "Loss within epoch 375:  10.344708442687988\n",
      "Loss within epoch 380:  10.160672187805176\n",
      "Loss within epoch 385:  9.982251167297363\n",
      "Loss within epoch 390:  9.809200286865234\n",
      "Loss within epoch 395:  9.641300201416016\n",
      "Loss within epoch 400:  9.478336334228516\n",
      "Loss within epoch 405:  9.320120811462402\n",
      "Loss within epoch 410:  9.166452407836914\n",
      "Loss within epoch 415:  9.0171537399292\n",
      "Loss within epoch 420:  8.872057914733887\n",
      "Loss within epoch 425:  8.730998039245605\n",
      "Loss within epoch 430:  8.593818664550781\n",
      "Loss within epoch 435:  8.460373878479004\n",
      "Loss within epoch 440:  8.330523490905762\n",
      "Loss within epoch 445:  8.204130172729492\n",
      "Loss within epoch 450:  8.081074714660645\n",
      "Loss within epoch 455:  7.961225509643555\n",
      "Loss within epoch 460:  7.844474792480469\n",
      "Loss within epoch 465:  7.730705738067627\n",
      "Loss within epoch 470:  7.619813442230225\n",
      "Loss within epoch 475:  7.511704444885254\n",
      "Loss within epoch 480:  7.406270980834961\n",
      "Loss within epoch 485:  7.303427696228027\n",
      "Loss within epoch 490:  7.2030863761901855\n",
      "Loss within epoch 495:  7.105155944824219\n",
      "Loss within epoch 500:  7.009561061859131\n",
      "Loss within epoch 505:  6.916221618652344\n",
      "Loss within epoch 510:  6.825068950653076\n",
      "Loss within epoch 515:  6.736024856567383\n",
      "Loss within epoch 520:  6.649025917053223\n",
      "Loss within epoch 525:  6.564002990722656\n",
      "Loss within epoch 530:  6.480898857116699\n",
      "Loss within epoch 535:  6.399641990661621\n",
      "Loss within epoch 540:  6.320188522338867\n",
      "Loss within epoch 545:  6.242473602294922\n",
      "Loss within epoch 550:  6.166446685791016\n",
      "Loss within epoch 555:  6.092055797576904\n",
      "Loss within epoch 560:  6.019253253936768\n",
      "Loss within epoch 565:  5.947991371154785\n",
      "Loss within epoch 570:  5.878220081329346\n",
      "Loss within epoch 575:  5.809904098510742\n",
      "Loss within epoch 580:  5.742992401123047\n",
      "Loss within epoch 585:  5.677449703216553\n",
      "Loss within epoch 590:  5.613229751586914\n",
      "Loss within epoch 595:  5.550307750701904\n",
      "Loss within epoch 600:  5.488633632659912\n",
      "Loss within epoch 605:  5.428181171417236\n",
      "Loss within epoch 610:  5.368913173675537\n",
      "Loss within epoch 615:  5.310793876647949\n",
      "Loss within epoch 620:  5.25379753112793\n",
      "Loss within epoch 625:  5.197891712188721\n",
      "Loss within epoch 630:  5.143045902252197\n",
      "Loss within epoch 635:  5.089233875274658\n",
      "Loss within epoch 640:  5.036426067352295\n",
      "Loss within epoch 645:  4.984594821929932\n",
      "Loss within epoch 650:  4.933720111846924\n",
      "Loss within epoch 655:  4.883771896362305\n",
      "Loss within epoch 660:  4.8347272872924805\n",
      "Loss within epoch 665:  4.786564350128174\n",
      "Loss within epoch 670:  4.739258289337158\n",
      "Loss within epoch 675:  4.692791938781738\n",
      "Loss within epoch 680:  4.647141933441162\n",
      "Loss within epoch 685:  4.602285385131836\n",
      "Loss within epoch 690:  4.558206558227539\n",
      "Loss within epoch 695:  4.514885425567627\n",
      "Loss within epoch 700:  4.4723029136657715\n",
      "Loss within epoch 705:  4.4304423332214355\n",
      "Loss within epoch 710:  4.389286041259766\n",
      "Loss within epoch 715:  4.348813056945801\n",
      "Loss within epoch 720:  4.309016227722168\n",
      "Loss within epoch 725:  4.269871234893799\n",
      "Loss within epoch 730:  4.2313642501831055\n",
      "Loss within epoch 735:  4.193485260009766\n",
      "Loss within epoch 740:  4.156216621398926\n",
      "Loss within epoch 745:  4.119542598724365\n",
      "Loss within epoch 750:  4.083453178405762\n",
      "Loss within epoch 755:  4.047933578491211\n",
      "Loss within epoch 760:  4.012970924377441\n",
      "Loss within epoch 765:  3.9785540103912354\n",
      "Loss within epoch 770:  3.9446680545806885\n",
      "Loss within epoch 775:  3.911302328109741\n",
      "Loss within epoch 780:  3.8784468173980713\n",
      "Loss within epoch 785:  3.846090316772461\n",
      "Loss within epoch 790:  3.814221143722534\n",
      "Loss within epoch 795:  3.7828292846679688\n",
      "Loss within epoch 800:  3.7519052028656006\n",
      "Loss within epoch 805:  3.721437692642212\n",
      "Loss within epoch 810:  3.691418409347534\n",
      "Loss within epoch 815:  3.6618359088897705\n",
      "Loss within epoch 820:  3.632683277130127\n",
      "Loss within epoch 825:  3.6039514541625977\n",
      "Loss within epoch 830:  3.5756311416625977\n",
      "Loss within epoch 835:  3.5477137565612793\n",
      "Loss within epoch 840:  3.520191192626953\n",
      "Loss within epoch 845:  3.493055582046509\n",
      "Loss within epoch 850:  3.466301202774048\n",
      "Loss within epoch 855:  3.4399166107177734\n",
      "Loss within epoch 860:  3.413896322250366\n",
      "Loss within epoch 865:  3.388232469558716\n",
      "Loss within epoch 870:  3.362922191619873\n",
      "Loss within epoch 875:  3.3379526138305664\n",
      "Loss within epoch 880:  3.313322067260742\n",
      "Loss within epoch 885:  3.289018392562866\n",
      "Loss within epoch 890:  3.2650394439697266\n",
      "Loss within epoch 895:  3.241380214691162\n",
      "Loss within epoch 900:  3.218031167984009\n",
      "Loss within epoch 905:  3.1949892044067383\n",
      "Loss within epoch 910:  3.172245979309082\n",
      "Loss within epoch 915:  3.1497976779937744\n",
      "Loss within epoch 920:  3.12764048576355\n",
      "Loss within epoch 925:  3.105766534805298\n",
      "Loss within epoch 930:  3.0841691493988037\n",
      "Loss within epoch 935:  3.062845230102539\n",
      "Loss within epoch 940:  3.0417916774749756\n",
      "Loss within epoch 945:  3.0210001468658447\n",
      "Loss within epoch 950:  3.0004711151123047\n",
      "Loss within epoch 955:  2.980195999145508\n",
      "Loss within epoch 960:  2.9601669311523438\n",
      "Loss within epoch 965:  2.940385580062866\n",
      "Loss within epoch 970:  2.920844793319702\n",
      "Loss within epoch 975:  2.9015402793884277\n",
      "Loss within epoch 980:  2.8824691772460938\n",
      "Loss within epoch 985:  2.863628387451172\n",
      "Loss within epoch 990:  2.84501051902771\n",
      "Loss within epoch 995:  2.8266148567199707\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Use New GloVe Embeddings to Get Predicted Word",
   "id": "534b9c3b8cd762dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T05:19:37.565409Z",
     "start_time": "2025-01-14T05:19:37.559046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctx_words = 'we conjure the of the computer'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model2(ctx_tensor)\n",
    "pred = get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ],
   "id": "92276088da3bfd28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word is: \"spirits\"\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
